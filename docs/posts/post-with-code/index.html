<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Zhu">
<meta name="dcterms.date" content="2025-03-13">
<meta name="description" content="üìÑpaper, üíªcode">

<title>MichaelZspace - A Scalable Framework to Decode Neural Representation of Social Interaction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">MichaelZspace</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Scalable Framework to Decode Neural Representation of Social Interaction</h1>
                  <div>
        <div class="description">
          <a href="https://drive.google.com/file/d/1PUhsLCAq-ZCMxXJlnDkSvAAtTbR8MA_P/view">üìÑpaper</a>, <a href="https://github.com/psymichaelzhu/Social_Interaction">üíªcode</a>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">active learning</div>
                <div class="quarto-category">multi-modal model</div>
                <div class="quarto-category">embedding</div>
                <div class="quarto-category">archetype-based method</div>
                <div class="quarto-category">neuroscience</div>
                <div class="quarto-category">computational approach</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Michael Zhu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 13, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning">Deep learning</a></li>
  <li><a href="#representational-similarity-analysis" id="toc-representational-similarity-analysis" class="nav-link" data-scroll-target="#representational-similarity-analysis">Representational similarity analysis</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#deep-learning-1" id="toc-deep-learning-1" class="nav-link" data-scroll-target="#deep-learning-1">Deep learning</a></li>
  <li><a href="#representational-similarity-analysis-1" id="toc-representational-similarity-analysis-1" class="nav-link" data-scroll-target="#representational-similarity-analysis-1">Representational similarity analysis</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In our daily lives, we constantly observe people interacting with one another. But how does our brain represent social interactions? To answer this question, previous studies have primarily used the naturalistic paradigm (<a href="https://arxiv.org/abs/2502.20349">Carvalho &amp; Lampinen, 2025</a>; <a href="https://doi.org/10.1109/tpami.2019.2901464">Monfort et al., 2020</a>) to investigate which aspects of social interactions are processed by different regions of the brain. For example, one study (<a href="https://doi.org/10.1016/j.cub.2023.10.015">McMahon et al., 2023</a>) demonstrated the existence of a social pathway that processes social interaction features in a hierarchical manner‚Äîranging from low-level attributes like agent distance to high-level aspects such as communication‚Äîmirroring the organization of basic visual processing (<a href="https://doi.org/10.1016/j.tics.2023.09.001">McMahon &amp; Isik, 2023</a>). Another study found that higher-level features of social interaction, such as mentalization, are represented in the superior temporal gyrus and middle temporal gyrus (<a href="https://doi.org/10.1093/scan/nsae030">Lee Masson et al., 2024</a>).</p>
<p>Traditionally, hypotheses are tested by comparing human annotations with neural activity to identify correlations between specific social interaction features and brain regions. The naturalistic paradigm is well-suited for this type of investigation, as its information-rich stimuli allow for the possibility of testing multiple hypotheses simultaneously (<a href="https://doi.org/10.1017/s0140525x22002874">Almaatouq et al., 2024</a>; <a href="https://arxiv.org/abs/2502.20349">Carvalho &amp; Lampinen, 2025</a>). However, due to financial constraints, human annotations are typically limited to a small set of dimensions, restricting our ability to fully utilize the richness of the stimuli and explore the design space comprehensively. To overcome this limitation, we propose a framework that leverages deep neural networks to efficiently generate hypothesis-based annotations, enabling the exploration of a much broader‚Äîpotentially infinite‚Äîrange of hypotheses.</p>
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 0.jpg" class="img-fluid figure-img" width="800"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Explore the Hypothesis Space (<a href="https://doi.org/10.1017/s0140525x22002874">Almaatouq et al., 2024</a>)</figcaption>
</figure>
</div>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<section id="participants-design" class="level4">
<h4 class="anchored" data-anchor-id="participants-design">Participants &amp; Design</h4>
<p>The dataset is from (<a href="https://doi.org/10.1016/j.cub.2023.10.015">McMahon et al., 2023</a>). The study included 4 participants (2 females, mean age 25.5 years, range 23‚Äì30; 3 Caucasian, 1 Asian), each undergoing four 2-hour fMRI sessions (3T, TR =1.5s). Participants viewed 250 muted 3-second videos depicting dyadic social interactions. The videos were selected from the <a href="http://moments.csail.mit.edu/">Moments in Time</a> dataset (<a href="https://doi.org/10.1109/tpami.2019.2901464">Monfort et al., 2020</a>) and annotated across low-, mid-, and high-level features, including visual, social, and affective properties.</p>
<div id="fig-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 1.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Video Distribution on Human-annotated Dimensions (<a href="https://doi.org/10.1109/tpami.2019.2901464">Monfort et al., 2020</a>)</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
fMRI data Preprocessing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the original paper, a systematic preprocessing has been performed using fMRIPrep (Esteban et al., 2019) based on Nipype 1.6.1(Gorgolewski et al., 2011). Key steps included:</p>
<ol type="1">
<li><p>Reference Volume &amp; Skull Stripping. A reference volume was generated by aligning and averaging single-band reference (SBRef) images, followed by skull stripping.</p></li>
<li><p>Motion Correction. Head motion parameters were estimated using mcflirt (Jenkinson et al., 2002), and images were realigned to correct for movement artifacts.</p></li>
<li><p>Field Map Correction. Estimated field maps were aligned with the echo-planar imaging (EPI) reference to correct for distortions.</p></li>
<li><p>Slice-Time Correction. BOLD images were temporally aligned using 3dTshift from AFNI (Cox &amp; Hyde, 1997).</p></li>
<li><p>Spatial Co-Registration. The BOLD reference was co-registered to the T1-weighted structural image using bbregister (FreeSurfer) with boundary-based registration (Greve &amp; Fischl, 2009).</p></li>
<li><p>Noise Reduction &amp; Confound Regressors. Several nuisance regressors were computed, including framewise displacement (FD), DVARS (Power et al., 2014), and global signals from cerebrospinal fluid (CSF), white matter (WM), and whole-brain masks. CompCor (Behzadi et al., 2007) was used to extract physiological noise components.</p></li>
<li><p>Motion Outlier Detection. High-motion frames were identified and flagged based on predefined FD and DVARS thresholds.</p></li>
<li><p>Spatial Normalization. BOLD images were resampled to MNI152NLin2009cAsym space for standardization.</p></li>
<li><p>Surface &amp; Volumetric Resampling. Functional data were mapped onto FreeSurfer‚Äôs fsnative space and volumetric data were transformed using ANTs‚Äô antsApplyTransforms with Lanczos interpolation (Lanczos, 1964).</p></li>
</ol>
<p>The regions of interest (ROIs) in this study follow those defined by McMahon et al.&nbsp;(2023). Anatomical ROIs, including EVC and MT, were identified based on structural landmarks. Anatomically constrained functional ROIs, such as pSTS and aSTS, were localized using functional tasks but constrained within anatomically defined regions. Functional ROIs, including biomotion-STS, TPJ, FFA, face-pSTS, PPA, EBA, and LOC, were identified purely through functional localizers without anatomical constraints.</p>
</div>
</div>
</div>
</section>
</section>
<section id="deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning">Deep learning</h3>
<p>Here we propose a framework utilizing deep learning to develop automated, hypothesis-based annotations for naturalistic stimuli such as videos.</p>
<p>Traditional naturalistic paradigms rely on human annotations to describe social interaction features in videos, but this approach is costly thus restricts the number of hypotheses that can be tested (<a href="https://doi.org/10.1017/s0140525x22002874">Almaatouq et al., 2024</a>). To overcome this, we use the Contrastive Language-Image Pre-training (<a href="https://openai.com/index/clip/">CLIP</a>) model (<a href="#fig-3">Figure 3</a>), a multi-modal model trained on paired image-text data (<a href="https://arxiv.org/abs/2103.00020">Radford et al., 2021</a>), to generate video annotations automatically, allowing for systematic, scalable, and fine-grained hypothesis testing (<a href="https://doi.org/10.1093/scan/nsae014">Sievers &amp; Thornton, 2024</a>). Instead of directly computing the similarity between video embeddings and raw concepts, this method operationalizes the target concept as a structured set of items, enabling a more nuanced evaluation of video content along specific dimensions (<a href="#fig-4">Figure 4</a>).</p>
<div id="fig-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 2.jpg" class="img-fluid figure-img" width="800"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Conceptual Introduction: Contrastive Language-Image Pre-training (CLIP) model <a href="https://arxiv.org/abs/2103.00020">Radford et al., 2021</a></figcaption>
</figure>
</div>
<p>For a given video, we extract a video embedding vector by first processing individual frames and then aggregating their embeddings. Each video is sliced into a sequence of 90 frames, and each frame is passed through CLIP‚Äôs vision encoder to obtain an embedding vector. The frame embeddings are then averaged to generate a single video embedding vector.</p>
<p>Since CLIP employs a shared embedding space for images and text, it allows direct comparisons between concept embeddings and video embeddings. However, Theoretical concepts are often <strong>multi-faceted</strong>, making a direct association between a video‚Äôs embedding and the original concept embedding difficult to interpret.</p>
<p>To address this, we adopt an approach inspired by questionnaire-based psychological measurement (<a href="https://doi.org/10.1038/s41562-024-02089-y">Wulff &amp; Mata, 2025</a>), where a hypothesis about a specific dimension of social interaction is first operationalized into a structured set of items. For instance, if the hypothesis is about the ‚Äú<em>relationship</em>‚Äù dimension of social interactions, we take specific relationship types such as ‚Äú<em>friends</em>‚Äù, ‚Äú<em>coworkers</em>‚Äù, as items to define this dimension. Each item is converted into an embedding vector using CLIP‚Äôs text encoder.</p>
<p>We then compute the cosine similarity between the video embedding and each item embedding. These similarity scores are normalized across items to sum to one, characterizing the relative likelihood of how well the video aligns with each item. The final vector provides a structured description of the video in a specific hypothesis context.</p>
<div id="fig-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 3.jpg" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Hypothesis-based Annotation Based on CLIP</figcaption>
</figure>
</div>
<p>After obtaining the annotation vector for each video, we can measure their pairwise similarity within the hypothesis dimension, by computing 1 <span class="math inline">\(-\)</span> Euclidean distance between their annotation vectors. For example, if one video has a higher score for ‚Äú<em>friends</em>‚Äù while another scores predominantly on ‚Äú<em>coworkers</em>‚Äù, their similarity within this ‚Äú<em>relationship</em>‚Äù dimension would be low.</p>
<p>This method automates video annotation, replacing costly human annotations with a scalable, hypothesis-driven labeling process. It captures video properties in a structured manner rather than relying on generic embedding semantic similarity. This allows us to test hypotheses about which social interaction attribute, like ‚Äú<em>relationship</em>‚Äù or ‚Äú<em>emotional valence</em>‚Äù, better explains neural activity, by analyzing the inter-video similarity patterns of these dimensions and comparing their explanatory power towards neural activity through <a href="#representational-similarity-analysis">Representational Similarity Analysis</a>.</p>
<p>In this demonstration, we picked several dimensions based on literature to annotate social interactions (<a href="https://doi.org/10.1038/s41562-025-02122-8">Cheng et al., 2025</a>; <a href="https://doi.org/10.1038/s44159-021-00008-w">Hadley et al., 2022</a>; <a href="https://doi.org/10.1016/j.tics.2023.09.001">McMahon &amp; Isik, 2023</a>; <a href="https://doi.org/10.1146/annurev-psych-081420-110718">van Dijk &amp; De Dreu, 2021</a>): <em>Valence</em> captures the emotional tone of the interaction (e.g., positive, negative). <em>Relationship</em> defines the social connection between agents (e.g., parent-child, couple, coworkers, friends, neighbors). <em>Activity</em> refers to the specific actions people are engaged in (e.g., people dancing, people playing sports, people playing instruments, people cooking, people fishing). <em>Communication modality</em> distinguishes different forms of information flow (e.g., verbal communication, non-verbal communication). <em>Demographics</em> account for the identity of the agents (e.g., male, female). <em>Context</em> describes the environment in which the interaction takes place (e.g., indoor, yard, wild). <em>Joint action</em> differentiates whether individuals coordinate their actions or act independently (e.g., joint action, independent action). <em>Transitivity</em> describes whether people interact with objects, other people, or act alone (e.g., people interacting with objects, people interacting with other people, people acting independently). <em>Engagement</em> reflects the level of involvement in the interaction (e.g., people showing high engagement, people showing low engagement).</p>
<p>For comparison purpose, we also extract the original embeddings from the <a href="https://github.com/facebookresearch/MetaCLIP/tree/main">CLIP model</a> (model name: ViT-bigG-14-quickgelu, pretrained weights: metaclip_fullcc), with length of 1280. In addition, we extract the global average pooling layer embeddings from two visual models: ResNet3d50, an inflated version of <a href="https://github.com/zhoubolei/moments_models">ResNet50</a> to deal with video (<a href="https://arxiv.org/abs/1705.07750">Carreira &amp; Zisserman, 2017</a>); multi-ResNet3d50, a refined ResNet3d50 trained on multiple overlapping actions using the Broden dataset and wLSEP loss (<a href="https://doi.org/10.1109/tpami.2019.2901464">Monfort et al., 2020</a>). The visual models yield embeddings of length 2048.</p>
</section>
<section id="representational-similarity-analysis" class="level3">
<h3 class="anchored" data-anchor-id="representational-similarity-analysis">Representational similarity analysis</h3>
<p>Representational Similarity Analysis (RSA) is used to compare the explanatory power of different hypothesis candidates towards neural activity pattern. (<a href="https://doi.org/10.3389/neuro.06.004.2008">Kriegeskorte, 2008</a>; <a href="https://doi.org/10.1371/journal.pcbi.1003553">Nili et al., 2014</a>; <a href="https://doi.org/10.1093/scan/nsz099">Popal et al., 2019</a>). The core idea is to quantify the similarity structure of representations by constructing representational similarity matrices (RSMs) and then compare candidate RSMs with neural RSM (<a href="#fig-6">Figure 6</a>).</p>
<div id="fig-5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 4.jpg" class="img-fluid figure-img" width="700"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Conceptual Introduction: Representational Similarity Analysis (<a href="https://doi.org/10.1093/scan/nsz099">Popal et al., 2019</a>)</figcaption>
</figure>
</div>
<p>For the neural RSM, we described the representational structure of each participant‚Äôs region-level neural activity during video viewing. This was done by extracting the voxel-level Œ≤-value sequences (activation levels, offered by the original paper) for each two videos in a given region and computing the pairwise Pearson correlation between their activation patterns. This resulted in a 244 √ó 244 similarity matrix, reflecting how similarly the region of this participant responded to each pair of videos. At the group level, we averaged across participants‚Äô RSMs to obtain a group neural RSM (<a href="https://doi.org/10.1101/2024.12.05.626975">Sartzetaki et al., 2024</a>). To ensure statistical robustness, we applied Fisher‚Äôs z-transformation (<a href="https://psycnet.apa.org/record/1987-14534-001">Silver &amp; Dunlap, 1987</a>) before averaging and converted the results back to correlation values.</p>
<p>To explain neural RSMs, we considered two types of candidate RSMs. The first type was based on deep learning model embeddings, where we extracted high-dimensional representations of pre-trained deep learning models for each video. Since embedding spaces are typically non-Euclidean (<a href="https://doi.org/10.1038/s41562-024-02089-y">Wulff &amp; Mata, 2025</a>), we computed cosine similarity between video embeddings to construct an embedding RSM of size 244 √ó 244 for each model. The second type relied on <a href="#fig-4">hypothesis-driven annotations</a>, where we extracted annotations for each video under specific hypothesis dimensions using the CLIP model. Since these annotation scores were normally distributed (see <a href="#fig-9">Figure 9</a>), we computed 1 - Euclidean distance between annotation vectors to measure similarity, yielding a 244 √ó 244 RSM for each hypothesis dimension. In total, we have 13 candidate RSMs: 3 model embedding RSMs and 10 dimensional annotation RSMs.</p>
<div id="fig-6" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 5.jpg" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Representational Similarity Analysis Roadmap in This Study</figcaption>
</figure>
</div>
<p>To assess the relationship between neural patterns and model-based representations, we computed Spearman correlations between the lower triangles of the neural RSM and each candidate RSM (<a href="https://doi.org/10.3389/neuro.06.004.2008">Kriegeskorte, 2008</a>; <a href="https://doi.org/10.1093/scan/nsz099">Popal et al., 2019</a>). This was done for each participant individually as well as at the group-level averaged RSMs. To determine statistical significance, we performed permutation testing of 1000 times by shuffling similarity values within the RSM (<a href="https://doi.org/10.1371/journal.pcbi.1003553">Nili et al., 2014</a>). This resulting the ROI x Candidate correlation matrices, for each subject and group-level.</p>
<p>To further quantify how different candidate RSMs explained neural similarity, we performed a regression analysis (<a href="https://doi.org/10.1038/s41562-017-0072">Parkinson et al., 2017</a>). First, we extracted the lower triangular part of each RSM, resulting in feature vectors of approximately 29,646 values per participant. These vectors included video-pairwise neural similarity, model-based similarity, and annotation-based similarity. We then modeled neural similarity as a function of the candidate RSM similarities using linear mixed-effects models, with random intercepts for each participant (not including group-level RSM) to account for individual variability (<a href="https://doi.org/10.1038/s41562-019-0800-6">Stolier et al., 2020</a>). To ensure comparability across different similarity metrics, we rank-transformed similarity values within each vector before running the regression, similar to the logic of Spearman correlation in correlation analysis (<a href="https://doi.org/10.1038/s41562-017-0072">Parkinson et al., 2017</a>).</p>
<p>The regression analysis provided coefficients quantifying the contribution of each candidate model to explaining neural representational structure. By comparing these coefficients, we assessed which candidate RSM best explained neural activity patterns while controlling for the effects of other candidates. Through this combination of RSA and regression modeling, we were able to systematically examine the alignment between neural representations, deep learning models, and hypothesis-driven annotations, offering insights into how different levels of computational representation relate to neural encoding of social interactions.</p>
<p>Considering the large volume of comparisons, all statistical significances are obtained after the stringent Bonferroni correction (<a href="https://www.bmj.com/content/310/6973/170">Bland &amp; Altman, 1995</a>).</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="deep-learning-1" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-1">Deep learning</h3>
<p>We first evaluated the validity of aggregating frame embeddings into video embeddings. As shown in the <a href="#fig-7">Figure 7</a>, the within-video similarity, measured as the cosine similarity between each frame and its video centroid, is consistently high (all above 0.95), while the between-video similarity, calculated as the cosine similarity between the centroids of different videos, is relatively lower and has larger variance across videos. This clear separation indicates that cross-frame aggregation should preserve video-level representation.</p>
<div id="fig-7" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 6.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;7: <strong>Verification: Cross-frame aggregation for obtaining video embeddings.</strong> This scatter plot (jittered by x-axis) illustrates the cosine similarity of within-video (blue) and between-video (orange) comparisons. The x-axis represents randomly selected video indices, while the y-axis shows cosine similarity values. After obtaining the video CLIP embeddings, we examined their consistency with human ratings.</figcaption>
</figure>
</div>
<p>After obtaining the video CLIP embeddings, we examined their consistency with human ratings <a href="#fig-8">Figure 8</a>. The results indicate that CLIP embedding effectively captures meaningful social interaction attributes, showing strong correlations in dimensions such as expanse, indoor, and agent distance (p&lt;0.001), while more abstract dimensions like joint action (p&lt;0.01), valence and arousal (p &lt; 0.05) exhibit weaker correlations, but all of them are statistically significant. This further validates the reliability of the video embeddings and demonstrates CLIP‚Äôs potential for capturing semantic information.</p>
<div id="fig-8" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 7.jpg" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">Figure&nbsp;8: <strong>Correlation Between Human Ratings and CLIP Annotations Across Dimensions.</strong> This figure shows the correlation between human rating scores and CLIP-derived annotation scores across various dimensions (McMahon et al., 2023). The annotation scores were obtained by using description of each dimension in original paper to generate embeddings, which were then compared to video embeddings to generate similarity scores. Each subplot represents a different dimension, with scatter points showing individual video scores and trend lines indicating correlation strength (shadow ribbon as 95% confidence interval). Asterisks (* &lt;0.05, ** &lt;0.01, *** &lt;0.001) denote levels of statistical significance.</figcaption>
</figure>
</div>
<p>Finally, we employed the framework proposed before to extract hypothesis-based annotation for videos.<br>
To have an intuitive sense of the dimensional annotation, <a href="#fig-9">Figure 9</a> is an example of annotations within the ‚Äú<em>game theory dynamics</em>‚Äù dimension, with three items: <em>cooperation, competition, and coordination</em>. The distribution of scores for each item, along with the highest- and lowest-scoring videos, demonstrates that the annotation process generally aligns with human intuition. The most striking point lies in the distinction between cooperation and coordination: High-scoring cooperation videos predominantly feature people working together towards a shared goal (e.g., planting trees), whereas coordination captures synchronized actions requiring precise temporal alignment (e.g., dancing, playing in a band). This distinction highlights the framework‚Äôs ability to capture subtle semantic differences, reinforcing its potential for video annotation.</p>
<div id="fig-9" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 8.jpg" class="img-fluid figure-img" width="800"></p>
<figcaption class="figure-caption">Figure&nbsp;9: <strong>Example of ‚ÄúGame Theory Dynamics‚Äù Dimension Annotation.</strong> This figure presents the annotation results of the ‚Äúgame theory dynamics‚Äù dimension, which includes three items: cooperation, competition, and coordination. The left column displays score distributions for each item, with example video frames overlaid at different percentiles along the distribution (2nd, 25th, 50th, 75th, 98th). The right column shows five highest-scoring videos (top row) and five lowest-scoring videos (bottom row) for each item, providing visual examples of how videos vary along these items.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
More examples of CLIP annotations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="fig-10" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="../../img/Figure S2a.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Example of ‚ÄúTransitivity‚Äù Dimension Annotation</figcaption>
</figure>
</div>
<div id="fig-11" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="../../img/Figure S2b.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;11: Example of ‚ÄúContext‚Äù Dimension Annotation</figcaption>
</figure>
</div>
<div id="fig-12" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="../../img/Figure S2c.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;12: Example of ‚ÄúActivity‚Äù Dimension Annotation</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="representational-similarity-analysis-1" class="level3">
<h3 class="anchored" data-anchor-id="representational-similarity-analysis-1">Representational similarity analysis</h3>
<p>In this section, we examine the relationship between different candidate RSMs and neural RSMs. Given the large number of videos, the number of video pairs is substantial, resulting in an RSM with a vast number of observations in its lower triangle. This high volume of data means that even minor effects can appear statistically significant (<a href="https://doi.org/10.4103/2229-3485.159943">Ranganathan et al., 2015</a>). For instance, at the group level, the neural-candidate correlations yielded 88.1% permutation-based significance, even after Bonferroni multiple comparison correction. Therefore, in interpreting and visualizing these results, we focus primarily on effect sizes‚Äîthe magnitude of correlation and regression coefficients‚Äîrather than statistical significance.</p>
<p>We start by calculating the Spearman correlation between neural and candidate RSMs <a href="#fig-13">Figure 13</a>. For the network embedding results (right 3 columns), CLIP embedding similarity patterns correlate a broader range of ROIs compared to purely visual models (ResNet variants). However, while CLIP embeddings outperform vision-based embeddings in explaining neural patterns, they still lack interpretability‚Äîmaking it difficult to pinpoint the specific dimensions driving these neural correlations. This highlights the value of dimensional annotations, which provide a more hypothesis-based, fine-grained understanding of the underlying representations.</p>
<div id="fig-13" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../../img/Figure 9.png" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;13: <strong>Spearman Correlation Between Neural and Candidate RSMs.</strong> This heatmap presents the Spearman correlation between ROI RSMs(y-axis) and candidate RSMs (x-axis). Candidate RSMs were derived from hypothesis-based annotations (the left section) network embeddings (the right section). Warmer colors indicate positive correlations, while cooler colors indicate negative correlations. Since most (88.1%) of correlations are significant through permutation test of 1000 times, we did not highlight based on significance, instead based on the absolute value of the correlation coefficient. Correlations with magnitude larger than 0.15 are highlighted with black-bordered squares.</figcaption>
</figure>
</div>
<p>In contrast to the broad associations found in original CLIP embedding, the dimensional annotation RSMs reveal selective neural correlations, where specific dimensions are linked to distinct brain regions rather than broadly distributed.</p>
<p>Transitivity (i.e., whether people interact with objects, others, or act independently, <a href="#fig-10">Figure 10</a> shows above-threshold correlations (r&gt;0.15) with both sides of EBA (extrastriate body area), left MT (middle temporal visual area), right biomotion-STS (superior temporal sulcus) and right pSTS (posterior superior temporal sulcus). Given their well-established roles in body perception and action understanding, these regions appear to be particularly sensitive to how an agent engages with its environment (<a href="https://doi.org/10.1093/cercor/bhu277">Wang et al., 2015</a>; <a href="https://doi.org/10.1093/cercor/bhv111">Deen et al., 2015</a>). Additionally, right TPJ (temporoparietal junction), a region often associated with theory of mind and perspective-taking (<a href="https://doi.org/10.1016/s1053-8119(03)00230-1">Saxe &amp; Kanwisher, 2003</a>), also shows notable correlation, suggesting a possible role in higher-order interpretation of social engagement.</p>
<p>Context (e.g., indoor vs.&nbsp;outdoor, <a href="#fig-11">Figure 11</a>) dimension shows remarkable correlations with visual scene-processing regions, particularly both sides of FFA (fusiform face area), right LOC (lateral occipital cortex) and right face-pSTS. Activity (e.g., dancing, cooking, <a href="#fig-12">Figure 12</a>) also shows moderate correlation with both sides of FFA. Interestingly, FFA, despite its primary role in face recognition (<a href="https://doi.org/10.1098/rstb.2006.1934">Kanwisher &amp; Yovel, 2006</a>), demonstrates strong correlations with scene-related dimensions. In contrast, PPA (parahippocampal place area), which is conventionally associated with scene processing (<a href="https://doi.org/10.1016/j.neuroimage.2012.02.055">Julian et al., 2012</a>), does not exhibit strong correlations with context.</p>
<p>A possible explanation for the FFA‚Äôs involvement is that different activities and contexts in these videos introduce substantial variations in facial visibility and orientation: ‚ÄúScene features in our dataset are also heavily confounded with the size and visibility of faces in our videos‚Äù (<a href="https://doi.org/10.1016/j.cub.2023.10.015">McMahon et al., 2023</a>). For example, in fishing scenes <a href="#fig-12">Figure 12</a>, individuals are often seen in profile, facing away from the camera, and appearing smaller in the frame, which could drive scene-related variability in face perception. In contrast, the lack of significant PPA correlations across both network embeddings and dimensional annotations suggests that its weak response is unlikely to be an artifact of the annotation framework. Instead, it may reflect a more fundamental limitation in the ability of deep models to capture PPA processing patterns.</p>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Driven by the question of <strong><em>which aspects of social interactions are represented by the brain</em></strong>, this study introduces a scalable, hypothesis-driven framework that automates video annotation, improving efficiency while maintaining interpretability. By integrating CLIP model embeddings with structured annotation design, this approach enables systematic exploration of the hypothesis space. Employing Representational Similarity Analysis, this study examines how neural responses align with different candidate representations, including CLIP and ResNet variants embeddings, as well as hypothesis-based annotations. The results reveal selective neural tuning, with distinct brain regions responding preferentially to specific features.</p>
<p>Several directions can further refine this framework:</p>
<ul>
<li>Although the alignment between CLIP annotations and neural activity as well as human ratings has been observed, a more comprehensive comparison with human judgments is needed to fully assess validity.</li>
<li>ROI definition in this study followed the original paper, which primarily focused on visual processing regions, potentially overlooking higher-order cognitive areas involved in social reasoning. Searchlight (<a href="https://doi.org/10.1073/pnas.0600244103">Kriegeskorte et al., 2006</a> analysis could provide a more comprehensive neural mapping.</li>
<li>This framework can be applied beyond social interaction, providing interpretable insights into broad perception and evaluation of complex stimuli, for example, which features of an image makes it memorable, or what factors drive the popularity of a YouTube video.</li>
<li>Finally, while this study used predefined annotation dimensions, the framework can be extended to automatically identify relevant dimensions using data-driven methods such as active learning within the hypothesis space (<a href="https://doi.org/10.1038/s41586-018-0637-6">Awad et al., 2018</a>; <a href="https://doi.org/10.1038/s41467-025-56700-5">Huang, 2025</a>; <a href="https://doi.org/10.1126/science.abe2629">Peterson et al., 2021</a>), which leverages previously explored hypotheses‚Äô results to guide the sampling of new dimensions.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Automated hypothesis discovery and testing to investigate Neural representations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>This framework provides an automated workflow to discover and test hypotheses about which dimensions of a complex stimuli (e.g., social interaction) correspond to their neural representations, shedding light on how human process how humans perceive and encode such information. It consists of two processes:</p>
<section id="constructing-the-design-space" class="level4">
<h4 class="anchored" data-anchor-id="constructing-the-design-space">Constructing the design space</h4>
<p>This process begins with relevant seed literature, which serves as the foundation for hypothesis generation. These sources are used as prompt examples for a Large Language Model agent, which undergoes multiple rounds of actor-critic refinement (Zhang et al., 2023) to iteratively expand and refine a hypothesis set. The final output is a structured list of potential dimensions, which are then embedded into a semantic space, forming the design space for further exploration.</p>
</section>
<section id="exploring-the-design-space-efficiently-through-an-active-learning-loop" class="level4">
<h4 class="anchored" data-anchor-id="exploring-the-design-space-efficiently-through-an-active-learning-loop">Exploring the design space efficiently through an active learning loop</h4>
<p>Initially, a subset of hypotheses is randomly sampled from the space, and Representational Similarity Analysis (RSA) is used to evaluate their correlation with neural activity patterns. Based on these results, a surrogate model is updated to estimate how different areas of the embedding space might relate to neural representations. A sampling strategy then selects the next set of hypotheses to test, balancing exploration (searching under-explored regions) and exploitation (prioritizing areas with strong neural correlations). This process repeats iteratively until the framework either achieves a comprehensive understanding of the entire space or identifies certain dimensions that exhibit strong and meaningful neural correlations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../img/Figure S1.jpg" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">Automatic Discovery and Testing of Hypotheses regarding Neural Representations</figcaption>
</figure>
</div>
<p>By integrating data-driven hypothesis generation with an iterative search process, this framework enables the automated specification of neural representation dimensions. Instead of relying on predefined annotations, it dynamically refines hypotheses, providing a structured yet flexible way to investigate how our brain represents complex stimuli.</p>
</section>
</div>
</div>
</div>
<hr>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>Almaatouq, A., Griffiths, T. L., Suchow, J. W., Whiting, M. E., Evans, J., &amp; Watts, D. J. (2024). Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences.<em>Behavioral and Brain Sciences, 47</em>. https://doi.org/10.1017/s0140525x22002874</p>
<p>Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., &amp; Rahwan, I. (2018). The Moral Machine experiment. <em>Nature, 563</em>(7729), 59‚Äì64. https://doi.org/10.1038/s41586-018-0637-6</p>
<p>Behzadi, Y., Restom, K., Liau, J., &amp; Liu, T. T. (2007). A component based noise correction method (CompCor) for BOLD and perfusion based fMRI. <em>Neuroimage, 37</em>(1), 90101.</p>
<p>Bland, J. M., &amp; Altman, D. G. (1995). Multiple significance tests: the Bonferroni method. <em>Bmj, 310</em>(6973), 170170.</p>
<p>Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. <em>Arxiv preprint arXiv:1705.07750</em>.</p>
<p>Carvalho, W., &amp; Lampinen, A. (2025). Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior. *Arxiv preprint arXiv:https://arxiv.org/abs/2502.20349</p>
<p>Cheng, X., Popal, H., Wang, H., Hu, R., Zang, Y., Zhang, M., Thornton, M. A., Cai, H., Bi, Y., Reilly, J., &amp; others. (2025). The Conceptual Structure of Human Relationships Across Modern and Historical Cultures. <em>Nature Human Behaviour</em>. https://doi.org/10.1038/s41562-024-01689-z</p>
<p>Cox, R. W., &amp; Hyde, J. S. (1997). Software tools for analysis and visualization of fMRI data. <em>NMR in Biomedicine: An International Journal Devoted to the Development and Application of Magnetic Resonance In Vivo, 10</em>(4-5), 171178.</p>
<p>Deen, B., Koldewyn, K., Kanwisher, N., &amp; Saxe, R. (2015). Functional Organization of Social Perception and Cognition in the Superior Temporal Sulcus. <em>Cerebral Cortex, 25</em>(11), 4596‚Äì4609. https://doi.org/10.1093/cercor/bhv111</p>
<p>Esteban, O., Markiewicz, C. J., Blair, R. W., Moodie, C. A., Isik, A. I., Erramuzpe, A., Kent, J. D., Goncalves, M., DuPre, E., Snyder, M., Oya, H., Ghosh, S. S., Wright, J., Durnez, J., Poldrack, R. A., &amp; Gorgolewski, K. J. (2019). fMRIPrep: a robust preprocessing pipeline for functional MRI. <em>Nature Methods, 16</em>(1), 111‚Äì116. https://doi.org/10.1038/s41592-018-0235-4</p>
<p>Gorgolewski, K., Burns, C. D., Madison, C., Clark, D., Halchenko, Y. O., Waskom, M. L., &amp; Ghosh, S. S. (2011). Nipype: A Flexible, Lightweight and Extensible Neuroimaging Data Processing Framework in Python. <em>Frontiers in Neuroinformatics, 5</em>. https://doi.org/10.3389/fninf.2011.00013</p>
<p>Greve, D. N., &amp; Fischl, B. (2009). Accurate and robust brain image alignment using boundary-based registration. <em>Neuroimage, 48</em>(1), 6372.</p>
<p>Hadley, L. V., Naylor, G., &amp; Hamilton, A. F. d.&nbsp;C. (2022). A review of theories and methods in the science of face-to-face social interaction. <em>Nature Reviews Psychology, 1</em>(1), 42‚Äì54. https://doi.org/10.1038/s44159-021-00008-w</p>
<p>Huang, L. (2025). Comprehensive exploration of visual working memory mechanisms using large-scale behavioral experiment. <em>Nature Communications, 16</em>(1). https://doi.org/10.1038/s41467-025-56700-5</p>
<p>Jenkinson, M., Bannister, P., Brady, M., &amp; Smith, S. (2002). Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images. <em>NeuroImage, 17</em>(2), 825‚Äì841. https://doi.org/10.1006/nimg.2002.1132</p>
<p>Julian, J., Fedorenko, E., Webster, J., &amp; Kanwisher, N. (2012). An algorithmic method for functionally defining regions of interest in the ventral visual pathway. <em>NeuroImage, 60</em>(4), 2357‚Äì2364. https://doi.org/10.1016/j.neuroimage.2012.02.055</p>
<p>Kanwisher, N., &amp; Yovel, G. (2006). The fusiform face area: a cortical region specialized for the perception of faces. <em>Philosophical Transactions of the Royal Society B: Biological Sciences, 361</em>(1476), 2109‚Äì2128. https://doi.org/10.1098/rstb.2006.1934</p>
<p>Kriegeskorte, N. (2008). Representational similarity analysis ‚Äì connecting the branches of systems neuroscience. <em>Frontiers in Systems Neuroscience</em>. https://doi.org/10.3389/neuro.06.004.2008</p>
<p>Kriegeskorte, N., Goebel, R., &amp; Bandettini, P. (2006). Information-based functional brain mapping. <em>Proceedings of the National Academy of Sciences, 103</em>(10), 3863‚Äì3868. https://doi.org/10.1073/pnas.0600244103</p>
<p>Lanczos, C. (1964). Evaluation of Noisy Data. <em>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis, 1</em>(1), 76‚Äì85. https://doi.org/10.1137/0701007</p>
<p>Lee Masson, H., Chang, L., &amp; Isik, L. (2024). Multidimensional neural representations of social features during movie viewing. <em>Social cognitive and affective neuroscience, 19</em>(1), nsae030. https://doi.org/10.1093/scan/nsae030</p>
<p>McMahon, E., Bonner, M. F., &amp; Isik, L. (2023). Hierarchical organization of social action features along the lateral visual pathway. <em>Current Biology, 33</em>(23), 5035‚Äì5047e8. https://doi.org/10.1016/j.cub.2023.10.015</p>
<p>McMahon, E., &amp; Isik, L. (2023). Seeing social interactions. <em>Trends in Cognitive Sciences, 27</em>(12), 1165‚Äì1179. https://doi.org/10.1016/j.tics.2023.09.001</p>
<p>Monfort, M., Vondrick, C., Oliva, A., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S. A., Yan, T., Brown, L., Fan, Q., &amp; Gutfreund, D. (2020). Moments in Time Dataset: One Million Videos for Event Understanding. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 42</em>(2), 502‚Äì508. https://doi.org/10.1109/tpami.2019.2901464</p>
<p>Nili, H., Wingfield, C., Walther, A., Su, L., Marslen-Wilson, W., &amp; Kriegeskorte, N. (2014). A Toolbox for Representational Similarity Analysis. <em>PLoS Computational Biology, 10</em>(4), e1003553. https://doi.org/10.1371/journal.pcbi.1003553</p>
<p>Parkinson, C., Kleinbaum, A. M., &amp; Wheatley, T. (2017). Spontaneous neural encoding of social network position. <em>Nature Human Behaviour, 1</em>(5). https://doi.org/10.1038/s41562-017-0072</p>
<p>Peterson, J. C., Bourgin, D. D., Agrawal, M., Reichman, D., &amp; Griffiths, T. L. (2021). Using large-scale experiments and machine learning to discover theories of human decision-making. <em>Science, 372</em>(6547), 1209‚Äì1214. https://doi.org/10.1126/science.abe2629</p>
<p>Popal, H., Wang, Y., &amp; Olson, I. R. (2019). A guide to representational similarity analysis for social neuroscience. <em>Social cognitive and affective neuroscience, 14</em>(11), 12431253.</p>
<p>Power, J. D., Mitra, A., Laumann, T. O., Snyder, A. Z., Schlaggar, B. L., &amp; Petersen, S. E. (2014). Methods to detect, characterize, and remove motion artifact in resting state fMRI. <em>Neuroimage, 84</em>, 320341.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ‚Ä¶ &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. <em>In International conference on machine learning (pp.&nbsp;8748-8763). PmLR</em>.</p>
<p>Ranganathan, P., Pramesh, C., &amp; Buyse, M. (2015). Common pitfalls in statistical analysis: Clinical versus statistical significance. <em>Perspectives in Clinical Research, 6</em>(3), 169. https://doi.org/10.4103/2229-3485.159943</p>
<p>Sartzetaki, C., Roig, G., Snoek, C. G., &amp; Groen, I. I. (2024). One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment. <em>Biorxiv, 202412</em>.</p>
<p>Saxe, R., &amp; Kanwisher, N. (2003). People thinking about thinking people: The role of the temporo-parietal junction in ‚Äútheory of mind.‚Äù <em>NeuroImage, 19</em>(4), 1835‚Äì1842. https://doi.org/10.1016/s1053-8119(03)00230-1</p>
<p>Seabold, S., &amp; Perktold, J. (2010). Statsmodels: econometric and statistical modeling with python. <em>SciPy, 7</em>(1), 92-96.</p>
<p>Sievers, B., &amp; Thornton, M. A. (2024). Deep social neuroscience: the promise and peril of using artificial neural networks to study the social brain. <em>Social cognitive and affective neuroscience, 19</em>(1), nsae014. https://doi.org/10.1093/scan/nsae014</p>
<p>Silver, N. C., &amp; Dunlap, W. P. (1987). Averaging correlation coefficients: should Fisher‚Äôs z transformation be used? <em>Journal of applied psychology, 72</em>(1), 146146.</p>
<p>Stolier, R. M., Hehman, E., &amp; Freeman, J. B. (2020). Trait knowledge forms a common structure across social cognition. <em>Nature Human Behaviour, 4</em>(4), 361‚Äì371. https://doi.org/10.1038/s41562-019-0800-6</p>
<p>van Dijk, E., &amp; De Dreu, C. K. (2021). Experimental Games and Social Decision Making. <em>Annual Review of Psychology, 72</em>(1), 415‚Äì438. https://doi.org/10.1146/annurev-psych-081420-110718</p>
<p>Wang, L., Mruczek, R. E., Arcaro, M. J., &amp; Kastner, S. (2015). Probabilistic Maps of Visual Topography in Human Cortex. <em>Cerebral Cortex, 25</em>(10), 3911‚Äì3931. https://doi.org/10.1093/cercor/bhu277</p>
<p>Wulff, D. U., &amp; Mata, R. (2025). Semantic embeddings reveal and address taxonomic incommensurability in psychological measurement. <em>Nature Human Behaviour</em>. https://doi.org/10.1038/s41562-024-02089-y</p>
<p>Zhang, B., Mao, H., Ruan, J., Wen, Y., Li, Y., Zhang, S., Xu, Z., Li, D., Li, Z., Zhao, R., &amp; others. (2023). Controlling large language model-based agents for large-scale decision-making: An actor-critic approach. <em>Arxiv preprint arXiv:2311.13884</em>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">¬© 2025 Welcome to MY COZY SPACE</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>