[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "A Scalable Framework to Decode Neural Representation of Social Interaction",
    "section": "",
    "text": "In our daily lives, we constantly observe people interacting with one another. But how does our brain represent social interactions? To answer this question, previous studies have primarily used the naturalistic paradigm (Carvalho & Lampinen, 2025; Monfort et al., 2020) to investigate which aspects of social interactions are processed by different regions of the brain. For example, one study (McMahon et al., 2023) demonstrated the existence of a social pathway that processes social interaction features in a hierarchical manner‚Äîranging from low-level attributes like agent distance to high-level aspects such as communication‚Äîmirroring the organization of basic visual processing (McMahon & Isik, 2023). Another study (Lee Masson et al., 2024) found that higher-level features of social interaction, such as mentalization, are represented in the superior temporal gyrus (STG) and middle temporal gyrus (MTG).\nTraditionally, hypotheses are tested by comparing human annotations with neural activity to identify correlations between specific social interaction features and brain regions. The naturalistic paradigm is well-suited for this type of investigation, as its information-rich stimuli allow for the possibility of testing multiple hypotheses simultaneously (Almaatouq et al., 2024; Carvalho & Lampinen, 2025). However, due to financial constraints, human annotations are typically limited to a small set of dimensions, restricting our ability to fully utilize the richness of the stimuli and explore the design space comprehensively. To overcome this limitation, we propose a framework that leverages deep neural networks to efficiently generate hypothesis-based annotations, enabling the exploration of a much broader‚Äîpotentially infinite‚Äîrange of hypotheses.\n\n\n\nFigure¬†1: Explore the Hypothesis Space (Almaatouq et al., 2024)"
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "A Scalable Framework to Decode Neural Representation of Social Interaction",
    "section": "",
    "text": "In our daily lives, we constantly observe people interacting with one another. But how does our brain represent social interactions? To answer this question, previous studies have primarily used the naturalistic paradigm (Carvalho & Lampinen, 2025; Monfort et al., 2020) to investigate which aspects of social interactions are processed by different regions of the brain. For example, one study (McMahon et al., 2023) demonstrated the existence of a social pathway that processes social interaction features in a hierarchical manner‚Äîranging from low-level attributes like agent distance to high-level aspects such as communication‚Äîmirroring the organization of basic visual processing (McMahon & Isik, 2023). Another study (Lee Masson et al., 2024) found that higher-level features of social interaction, such as mentalization, are represented in the superior temporal gyrus (STG) and middle temporal gyrus (MTG).\nTraditionally, hypotheses are tested by comparing human annotations with neural activity to identify correlations between specific social interaction features and brain regions. The naturalistic paradigm is well-suited for this type of investigation, as its information-rich stimuli allow for the possibility of testing multiple hypotheses simultaneously (Almaatouq et al., 2024; Carvalho & Lampinen, 2025). However, due to financial constraints, human annotations are typically limited to a small set of dimensions, restricting our ability to fully utilize the richness of the stimuli and explore the design space comprehensively. To overcome this limitation, we propose a framework that leverages deep neural networks to efficiently generate hypothesis-based annotations, enabling the exploration of a much broader‚Äîpotentially infinite‚Äîrange of hypotheses.\n\n\n\nFigure¬†1: Explore the Hypothesis Space (Almaatouq et al., 2024)"
  },
  {
    "objectID": "posts/post-with-code/index.html#methods",
    "href": "posts/post-with-code/index.html#methods",
    "title": "A Scalable Framework to Decode Neural Representation of Social Interaction",
    "section": "Methods",
    "text": "Methods\n\nDataset\n\nParticipants & Design\nThe dataset is from (McMahon et al., 2023). The study included 4 participants (2 females, mean age 25.5 years, range 23‚Äì30; 3 Caucasian, 1 Asian), each undergoing four 2-hour fMRI sessions (3T, TR =1.5s). Participants viewed 250 3-second videos depicting dyadic social interactions without sound. The videos were selected from the Moments in Time dataset (Monfort et al., 2020) and annotated across low-, mid-, and high-level features, including visual, social, and affective properties.\n\n\n\nFigure¬†2: Video Distribution on Annotated Dimensions (Monfort et al., 2020)\n\n\n\n\n\n\n\n\nfMRI data Preprocessing\n\n\n\n\n\nIn the original paper, a systematic preprocessing has been performed using fMRIPrep (Esteban et al., 2019) based on Nipype 1.6.1(Gorgolewski et al., 2011). Key steps included:\n\nReference Volume & Skull Stripping. A reference volume was generated by aligning and averaging single-band reference (SBRef) images, followed by skull stripping.\nMotion Correction. Head motion parameters were estimated using mcflirt (Jenkinson et al., 2002), and images were realigned to correct for movement artifacts.\nField Map Correction. Estimated field maps were aligned with the echo-planar imaging (EPI) reference to correct for distortions.\nSlice-Time Correction. BOLD images were temporally aligned using 3dTshift from AFNI (Cox & Hyde, 1997).\nSpatial Co-Registration. The BOLD reference was co-registered to the T1-weighted structural image using bbregister (FreeSurfer) with boundary-based registration (Greve & Fischl, 2009).\nNoise Reduction & Confound Regressors. Several nuisance regressors were computed, including framewise displacement (FD), DVARS (Power et al., 2014), and global signals from cerebrospinal fluid (CSF), white matter (WM), and whole-brain masks. CompCor (Behzadi et al., 2007) was used to extract physiological noise components.\nMotion Outlier Detection. High-motion frames were identified and flagged based on predefined FD and DVARS thresholds.\nSpatial Normalization. BOLD images were resampled to MNI152NLin2009cAsym space for standardization.\nSurface & Volumetric Resampling. Functional data were mapped onto FreeSurfer‚Äôs fsnative space and volumetric data were transformed using ANTs‚Äô antsApplyTransforms with Lanczos interpolation (Lanczos, 1964).\n\nThe regions of interest (ROIs) in this study follow those defined by McMahon et al.¬†(2023). Anatomical ROIs, including EVC and MT, were identified based on structural landmarks. Anatomically constrained functional ROIs, such as pSTS and aSTS, were localized using functional tasks but constrained within anatomically defined regions. Functional ROIs, including biomotion-STS, TPJ, FFA, face-pSTS, PPA, EBA, and LOC, were identified purely through functional localizers without anatomical constraints.\n\n\n\n\n\n\nDeep learning\nHere we propose a framework utilizing deep learning to develop automated, hypothesis-based annotations for naturalistic stimuli such as videos.\nTraditional naturalistic paradigms rely on human annotations to describe social interaction features in videos, but this approach is costly thus restricts the number of hypotheses that can be tested (Almaatouq et al., 2024). To overcome this, we use the CLIP model, a multi-modal model trained on paired image-text data (Radford et al., 2021), to generate video annotations automatically, allowing for systematic, scalable, and fine-grained hypothesis testing (Sievers & Thornton, 2024). Instead of directly computing the similarity between video embeddings and raw concepts, this method operationalizes the target concept as a structured set of items, enabling a more nuanced evaluation of video content along specific dimensions (Figure 4).\n\n\n\nFigure¬†3: Conceptual Introduction: Contrastive Language-Image Pre-training (CLIP) model Radford et al., 2021\n\n\nFor a given video, we extract a video embedding vector by first processing individual frames and then aggregating their embeddings. Each video is sliced into a sequence of frames, and each frame is passed through CLIP‚Äôs vision encoder to obtain an embedding vector. The frame embeddings are then aggregated using a mean pooling operation to generate a single video embedding vector.\nInspired by the idea of questionnaire-based psychological measurement, to annotate a video based on a hypothesis regarding certain dimension of social interaction, we first operationalize the hypothesis into a structured set of items. For example, if the hypothesis is about the ‚Äúagents‚Äô relationship‚Äù dimension of social interactions, we define specific relationship types such as ‚Äúfriends‚Äù, ‚Äúcoworkers‚Äù as items to define this dimension. Each item is converted into an embedding vector using CLIP‚Äôs text encoder.\nOnce both video embeddings and hypothesis item embeddings are obtained, we compute how well the video aligns with each item under the hypothesis dimension using cosine similarity. The similarity score between the video embedding and each item embedding is calculated, resulting in a score vector. These scores are then normalized across all items so that they sum to one, ensuring a relative weighting of how the video fits within the hypothesis dimension. This normalized vector provides a structured description of the video under a specific hypothesis focus.\n\n\n\nFigure¬†4: Hypothesis-based Annotation Based on CLIP\n\n\nAfter obtaining an annotation vector for each video, we can compare videos within a hypothesis dimension by computing the similarity between their annotation vectors. Given two videos, their similarity is computed using 1- Euclidean distance, which measures how similarly two videos are annotated within the hypothesis dimension. If two videos have high similarity, it suggests that they are similar under the given hypothesis. If they differ in certain items, such as one scoring high on ‚Äúfriends‚Äù and another on ‚Äúcoworkers,‚Äù they are more distinct in the ‚Äúagents‚Äô relationship‚Äù dimension.\nThis method automates video annotation, replacing costly human annotations with a scalable, hypothesis-driven labeling process. It captures video properties in a structured manner rather than relying on generic embedding semantic similarity. With precise comparisons between videos based on specific dimensions of interest, we can test different hypotheses, such as determining which representation structure, ‚Äúagent relationship‚Äù or ‚Äúemotional value,‚Äù can better explain the neural activity of a brain region, by comparing the similarity matrices derived from two annotation dimensions.\nIn this demonstration, we selected several dimensions to describe social interactions from literature (Cheng et al., 2025; Hadley et al., 2022; McMahon & Isik, 2023). Valence captures the emotional tone of the interaction (e.g., positive, negative). Relationship defines the social connection between individuals (e.g., parent-child, couple, coworkers, friends, neighbors). Activity refers to the specific actions people are engaged in (e.g., people dancing, people playing sports, people playing instruments, people cooking, people fishing). Communication modality distinguishes between different ways of information flow (e.g., verbal communication, non-verbal communication). Demographics account for the identity of the individuals involved (e.g., male, female, child). Context describes the environment in which the interaction takes place (e.g., indoor, yard, wild). Joint action differentiates whether individuals coordinate their actions or act independently (e.g., joint action, independent action). Transitivity describes whether people interact with objects, other people, or act alone (e.g., people interacting with objects, people interacting with other people, people acting independently). Engagement reflects the level of involvement in the interaction (e.g., people showing high engagement, people showing low engagement).\nFor comparison purpose, we also extract the original embedding of the CLIP model (model name: ViT-bigG-14-quickgelu, pretrained weights: metaclip_fullcc), with a length of 1280. In addition, we extract the global average pooling layer embeddings from several visual models, two ResNet50 variants, ResNet3d50, an inflated version to deal with video (Carreira & Zisserman, 2017), and multi-ResNet3d50, a version trained on multiple overlapping actions using the Broden dataset and wLSEP loss (Monfort et al., 2020). The visual models have embeddings with length 2048.\n\n\nRepresentational similarity analysis\nRepresentational Similarity Analysis (RSA) was used to compare neural activity patterns with different computational candidates (Kriegeskorte, 2008; Nili et al., 2014; Popal et al., 2019). The core idea is to quantify the similarity structure of representations by constructing representational similarity matrices (RSMs) and then compare candidate RSMs (models) with reference RSM (neural) (Figure 6).\n\n\n\nFigure¬†5: Conceptual Introduction: Representational Similarity Analysis (Popal et al., 2019)\n\n\nFor the neural RSM, we calculated the representational structure of each participant‚Äôs brain activity during video viewing. This was done by extracting the voxel-level Œ≤-value sequences (activation levels, organized by the original paper) for each two videos in a given ROI and computing the pairwise Pearson correlation between these activation patterns. This resulted in a 244 √ó 244 similarity matrix for each participant, reflecting how similarly the ROI responded to each pair of videos. At the group level, we averaged across participants‚Äô RSMs to obtain a group neural RSM (Sartzetaki et al., 2024). To ensure statistical robustness, we applied Fisher‚Äôs z-transformation (Silver & Dunlap, 1987) before averaging and converted the results back to correlation values.\nTo compare neural RSMs with computational models, we considered two types of candidate RSMs. The first type was based on network embeddings, where we extracted high-dimensional representations for each video using pre-trained deep learning models. Since embedding spaces are typically non-Euclidean (Wulff & Mata, 2025), we computed cosine similarity between video embeddings to construct an embedding RSM of size 244 √ó 244. The second type relied on hypothesis-driven annotations, where we extracted semantic annotations for each video using CLIP model. Since these annotation values were approximately normally distributed, we computed 1 - Euclidean distance between annotation vectors to measure similarity, yielding another 244 √ó 244 RSM.\n\n\n\nFigure¬†6: Representational Similarity Analysis Roadmap\n\n\nTo assess the relationship between neural patterns and model-based representations, we computed Spearman correlations between the lower triangles of the neural RSM and each candidate RSM (Kriegeskorte, 2008; Popal et al., 2019). This was done for each participant individually as well as at the group-level averaged RSMs. To determine statistical significance, we performed permutation testing of 1000 times by shuffling similarity values within the RSM (Nili et al., 2014). This resulting the correlation ROI x Candidate matrices, for each subject or at the group-level.\nTo further quantify how different candidate RSMs explained neural similarity, we performed a regression analysis (Parkinson et al., 2017). First, we extracted the lower triangular part of each RSM, resulting in feature vectors of approximately 29,646 values per participant. These vectors included video-pairwise neural similarity, model-based similarity, and annotation-based similarity. We then modeled neural similarity as a function of the candidate RSM similarities using linear mixed-effects models, with random intercepts for each participant (not including group-level RSM) to account for individual variability (Stolier et al., 2020). To ensure comparability across different similarity metrics, we rank-transformed similarity values within each vector before running the regression, similar to the logic of Spearman correlation in RSA (Parkinson et al., 2017).\nThe regression analysis provided Œ≤ coefficients quantifying the contribution of each candidate model to explaining neural representational structure. By comparing these Œ≤ values, we assessed which candidate RSM best explained neural activity patterns while controlling for the effects of other candidates. Through this combination of RSA and regression modeling, we were able to systematically examine the alignment between neural representations, deep learning models, and hypothesis-driven annotations, offering insights into how different levels of computational representation relate to neural encoding of social interactions. All statistical significances are obtained after the stringent Bonferroni multiple comparison correction (Bland & Altman, 1995)."
  },
  {
    "objectID": "posts/post-with-code/index.html#results",
    "href": "posts/post-with-code/index.html#results",
    "title": "A Scalable Framework to Decode Neural Representation of Social Interaction",
    "section": "Results",
    "text": "Results\n\nDeep learning\nWe first evaluated the validity of aggregating frame embeddings into video embeddings. As shown in the Figure 7, the within-video similarity, measured as the cosine similarity between each frame and its video centroid, is consistently high (all above 0.95), while the between-video similarity, calculated as the cosine similarity between the centroids of different videos, is relatively lower and has larger variance across videos. This clear separation indicates that cross-frame aggregation should preserve video-level representation.\n\n\n\nFigure¬†7: Verification: Cross-frame aggregation for obtaining video embeddings. This scatter plot (jittered by x-axis) illustrates the cosine similarity of within-video (blue) and between-video (orange) comparisons. The x-axis represents randomly selected video indices, while the y-axis shows cosine similarity values. After obtaining the video CLIP embeddings, we examined their consistency with human ratings.\n\n\nAfter obtaining the video CLIP embeddings, we examined their consistency with human ratings Figure 8. The results indicate that CLIP embedding effectively captures meaningful social interaction attributes, showing strong correlations in dimensions such as expanse, indoor, and agent distance (p&lt;0.001), while more abstract dimensions like joint action (p&lt;0.01), valence and arousal (p &lt; 0.05) exhibit weaker correlations, but all of them are statistically significant. This further validates the reliability of the video embeddings and demonstrates CLIP‚Äôs potential for capturing semantic information.\n\n\n\nFigure¬†8: Correlation Between Human Ratings and CLIP Annotations Across Dimensions. This figure shows the correlation between human rating scores and CLIP-derived annotation scores across various dimensions (McMahon et al., 2023). The annotation scores were obtained by using description of each dimension in original paper to generate embeddings, which were then compared to video embeddings to generate similarity scores. Each subplot represents a different dimension, with scatter points showing individual video scores and trend lines indicating correlation strength (shadow ribbon as 95% confidence interval). Asterisks (* &lt;0.05, ** &lt;0.01, *** &lt;0.001) denote levels of statistical significance.\n\n\nFinally, we employed the framework proposed before to extract hypothesis-based annotation for videos.\nTo have an intuitive sense of the dimensional annotation, Figure 9 is an example of annotations within the ‚Äúgame theory dynamics‚Äù dimension, with three items: cooperation, competition, and coordination. The distribution of scores for each item, along with the highest- and lowest-scoring videos, demonstrates that the annotation process generally aligns with human intuition. The most striking point lies in the distinction between cooperation and coordination: High-scoring cooperation videos predominantly feature people working together towards a shared goal (e.g., planting trees), whereas coordination captures synchronized actions requiring precise temporal alignment (e.g., dancing, playing in a band). This distinction highlights the framework‚Äôs ability to capture subtle semantic differences, reinforcing its potential for video annotation.\n\n\n\nFigure¬†9: Example of ‚ÄúGame Theory Dynamics‚Äù Dimension Annotation. This figure presents the annotation results of the ‚Äúgame theory dynamics‚Äù dimension, which includes three items: cooperation, competition, and coordination. The left column displays score distributions for each item, with example video frames overlaid at different percentiles along the distribution (2nd, 25th, 50th, 75th, 98th). The right column shows five highest-scoring videos (top row) and five lowest-scoring videos (bottom row) for each item, providing visual examples of how videos vary along these items.\n\n\n\n\n\n\n\n\nMore examples of CLIP annotations\n\n\n\n\n\n\n\n\nFigure¬†10: Example of ‚ÄúTransitivity‚Äù Dimension Annotation\n\n\n\n\n\nFigure¬†11: Example of ‚ÄúContext‚Äù Dimension Annotation\n\n\n\n\n\nFigure¬†12: Example of ‚ÄúActivity‚Äù Dimension Annotation\n\n\n\n\n\n\n\nRepresentational similarity analysis\nIn this section, we examine the relationship between different candidate RSMs and neural RSMs. Given the large number of videos, the number of video pairs is substantial, resulting in an RSM with a vast number of observations in its lower triangle. This high volume of data means that even minor effects can appear statistically significant (Ranganathan et al., 2015). For instance, at the group level, the neural-candidate correlations yielded 88.1% permutation-based significance, even after Bonferroni multiple comparison correction. Therefore, in interpreting and visualizing these results, we focus primarily on effect sizes‚Äîthe magnitude of correlation and regression coefficients‚Äîrather than statistical significance.\nWe start by calculating the Spearman correlation between neural and candidate RSMs Figure 13. For the network embedding results (right 3 columns), CLIP embedding similarity patterns correlate a broader range of ROIs compared to purely visual models (ResNet variants). However, while CLIP embeddings outperform vision-based embeddings in explaining neural patterns, they still lack interpretability‚Äîmaking it difficult to pinpoint the specific dimensions driving these neural correlations. This highlights the value of dimensional annotations, which provide a more hypothesis-based, fine-grained understanding of the underlying representations.\n\n\n\nFigure¬†13: Spearman Correlation Between Neural and Candidate RSMs. This heatmap presents the Spearman correlation between ROI RSMs(y-axis) and candidate RSMs (x-axis). Candidate RSMs were derived from hypothesis-based annotations (the left section) network embeddings (the right section). Warmer colors indicate positive correlations, while cooler colors indicate negative correlations. Since most (88.1%) of correlations are significant through permutation test of 1000 times, we did not highlight based on significance, instead based on the absolute value of the correlation coefficient. Correlations with magnitude larger than 0.15 are highlighted with black-bordered squares.\n\n\nIn contrast to the broad associations found in original CLIP embedding, the dimensional annotation RSMs reveal selective neural correlations, where specific dimensions are linked to distinct brain regions rather than broadly distributed.\nTransitivity (i.e., whether people interact with objects, others, or act independently, Figure 10 shows above-threshold correlations (r&gt;0.15) with both sides of EBA (extrastriate body area), left MT (middle temporal visual area), right biomotion-STS (superior temporal sulcus) and right pSTS (posterior superior temporal sulcus). Given their well-established roles in body perception and action understanding, these regions appear to be particularly sensitive to how an agent engages with its environment (Wang et al., 2015; Deen et al., 2015). Additionally, right TPJ (temporoparietal junction), a region often associated with theory of mind and perspective-taking (Saxe & Kanwisher, 2003), also shows notable correlation, suggesting a possible role in higher-order interpretation of social engagement.\nContext (e.g., indoor vs.¬†outdoor, Figure 11) dimension shows remarkable correlations with visual scene-processing regions, particularly both sides of FFA (fusiform face area), right LOC (lateral occipital cortex) and right face-pSTS. Activity (e.g., dancing, cooking, Figure 12) also shows moderate correlation with both sides of FFA. Interestingly, FFA, despite its primary role in face recognition (Kanwisher & Yovel, 2006), demonstrates strong correlations with scene-related dimensions. In contrast, PPA (parahippocampal place area), which is conventionally associated with scene processing (Julian et al., 2012), does not exhibit strong correlations with context.\nA possible explanation for the FFA‚Äôs involvement is that different activities and contexts in these videos introduce substantial variations in facial visibility and orientation: ‚ÄúScene features in our dataset are also heavily confounded with the size and visibility of faces in our videos‚Äù (McMahon et al., 2023). For example, in fishing scenes Figure 12, individuals are often seen in profile, facing away from the camera, and appearing smaller in the frame, which could drive scene-related variability in face perception. In contrast, the lack of significant PPA correlations across both network embeddings and dimensional annotations suggests that its weak response is unlikely to be an artifact of the annotation framework. Instead, it may reflect a more fundamental limitation in the ability of deep models to capture PPA processing patterns."
  },
  {
    "objectID": "posts/post-with-code/index.html#discussion",
    "href": "posts/post-with-code/index.html#discussion",
    "title": "A Scalable Framework to Decode Neural Representation of Social Interaction",
    "section": "Discussion",
    "text": "Discussion\nDriven by the question of which aspects of social interactions are represented by the brain, this study introduces a scalable, hypothesis-driven framework that automates video annotation, improving efficiency while maintaining interpretability. By integrating CLIP model embeddings with structured annotation design, this approach enables systematic exploration of the hypothesis space. Employing Representational Similarity Analysis, this study examines how neural responses align with different candidate representations, including CLIP and ResNet variants embeddings, as well as hypothesis-based annotations. The results reveal selective neural tuning, with distinct brain regions responding preferentially to specific features.\nSeveral directions can further refine this framework:\n\nAlthough the alignment between CLIP annotations and neural activity as well as human ratings has been observed, a more comprehensive comparison with human judgments is needed to fully assess validity.\nROI definition in this study followed the original paper, which primarily focused on visual processing regions, potentially overlooking higher-order cognitive areas involved in social reasoning. Searchlight (Kriegeskorte et al., 2006 analysis could provide a more comprehensive neural mapping.\nThis framework can be applied beyond social interaction, providing interpretable insights into broad perception and evaluation of complex stimuli, for example, which features of an image makes it memorable, or what factors drive the popularity of a YouTube video.\nFinally, while this study used predefined annotation dimensions, the framework can be extended to automatically identify relevant dimensions using data-driven methods such as active learning within the hypothesis space (Awad et al., 2018; Huang, 2025; Peterson et al., 2021), which leverages previously explored hypotheses‚Äô results to guide the sampling of new dimensions.\n\n\n\n\n\n\n\nAutomated hypothesis discovery and testing to investigate Neural representations\n\n\n\n\n\nThis framework provides an automated workflow to discover and test hypotheses about which dimensions of a complex stimuli (e.g., social interaction) correspond to their neural representations, shedding light on how human process how humans perceive and encode such information. It consists of two processes:\n\nConstructing the design space\nThis process begins with relevant seed literature, which serves as the foundation for hypothesis generation. These sources are used as prompt examples for a Large Language Model agent, which undergoes multiple rounds of actor-critic refinement (Zhang et al., 2023) to iteratively expand and refine a hypothesis set. The final output is a structured list of potential dimensions, which are then embedded into a semantic space, forming the design space for further exploration.\n\n\nExploring the design space efficiently through an active learning loop\nInitially, a subset of hypotheses is randomly sampled from the space, and Representational Similarity Analysis (RSA) is used to evaluate their correlation with neural activity patterns. Based on these results, a surrogate model is updated to estimate how different areas of the embedding space might relate to neural representations. A sampling strategy then selects the next set of hypotheses to test, balancing exploration (searching under-explored regions) and exploitation (prioritizing areas with strong neural correlations). This process repeats iteratively until the framework either achieves a comprehensive understanding of the entire space or identifies certain dimensions that exhibit strong and meaningful neural correlations.\n\n\n\nAutomatic Discovery and Testing of Hypotheses regarding Neural Representations\n\n\nBy integrating data-driven hypothesis generation with an iterative search process, this framework enables the automated specification of neural representation dimensions. Instead of relying on predefined annotations, it dynamically refines hypotheses, providing a structured yet flexible way to investigate how our brain represents complex stimuli."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MichaelZspace",
    "section": "",
    "text": "A Scalable Framework to Decode Neural Representation of Social Interaction\n\n\n\n\n\n\n\nactive learning\n\n\nmulti-modal model\n\n\nembedding\n\n\narchetype-based method\n\n\nneuroscience\n\n\ncomputational approach\n\n\n\n\nüìÑpaper, üíªcode\n\n\n\n\n\n\nMar 13, 2025\n\n\nMichael Zhu\n\n\n\n\n\n\nNo matching items"
  }
]